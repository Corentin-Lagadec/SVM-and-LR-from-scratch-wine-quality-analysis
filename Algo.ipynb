{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d545fab0",
   "metadata": {},
   "source": [
    "## File Structure and Design Choices\n",
    "\n",
    "In this file, you will find the implementation of three main classes: `Algo`, `SVM`, and `LogisticRegression`. \n",
    "\n",
    "- The `Algo` class serves as a base class that contains common functionality and parameters shared by different machine learning algorithms.\n",
    "- The `SVM` class inherits from `Algo` and implements the specifics of the Support Vector Machine algorithm.\n",
    "- The `LogisticRegression` class also inherits from `Algo` and contains the logic specific to logistic regression.\n",
    "\n",
    "### Justification of Design Choices\n",
    "\n",
    "The decision to have `SVM` and `LogisticRegression` inherit from a common base class `Algo` promotes code reuse and modularity. It allows us to centralize shared components such as regularization parameters, kernel functions, and evaluation metrics, which reduces duplication and improves maintainability.\n",
    "\n",
    "By grouping all related classes in a single file, we simplify the project structure, making it easier to understand and maintain. This approach also streamlines future extensions since new algorithms can be added by inheriting from `Algo`, ensuring consistency across implementations.\n",
    "\n",
    "Overall, this design enhances clarity, encourages clean architecture, and supports scalable development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50df5cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53dcd9",
   "metadata": {},
   "source": [
    "# Implementation of the base class 'Algo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15185cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algo:\n",
    "    def __init__(self, lambda_=0.01, max_iter=1000, size_subset = 1, tol=1e-2, kernel=None, rbf_param=2, degree= 2,track=False, step_iter=10, metric=metrics.accuracy):\n",
    "        # Initialization of common parameters for all algorithms\n",
    "        self.lambda_ = lambda_ #regularization term\n",
    "        self.max_iter = max_iter  # maximun of iterations\n",
    "        self.size_subset = size_subset # size of the  random subset (mini-batch) <= number of examples in the data set S\n",
    "        self.tol = tol # stop tolerance \n",
    "        self.kernel = kernel # kernelized form or not \n",
    "        self.rbf_param = rbf_param # \"sigma\" for kernel = \"rbf\"\n",
    "        self.degree =  degree # degree for \"poly\" kernel\n",
    "        self.track = track   # if activated draw the evolution of loss and a metric function evolution\n",
    "        self.step_iter = step_iter # step for the track of loss and a metric function evolution\n",
    "        self.metric = metric    # metric used for the plot\n",
    "        \n",
    "        # Internal attributes initialized later\n",
    "        self.alpha = None      # value for each support vector\n",
    "        self.losses = []       # track of the loss\n",
    "        self.metric_values = [] # track of the metric \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.gram_matrix = None\n",
    "        \n",
    "    # Linear kernel: dot product\n",
    "    def linear_kernel(self, x1, x2):\n",
    "        return np.dot(x1, x2)\n",
    "\n",
    "    # Polynomial kernel\n",
    "    def poly_kernel(self, x1, x2):\n",
    "        return (1 +np.dot(x1,x2)) ** self.degree\n",
    "    \n",
    "    # RBF (Gaussian) kernel\n",
    "    def rbf_kernel(self, x1, x2):\n",
    "        return np.exp(-np.linalg.norm(x1 - x2) ** 2 / (2 * self.rbf_param ** 2))\n",
    "    \n",
    "    # Compute the kernel between two vectors\n",
    "    def compute_kernel(self, x1, x2):\n",
    "        if self.kernel == \"linear\":\n",
    "            return self.linear_kernel(x1, x2)\n",
    "        elif self.kernel == \"poly\":\n",
    "            return self.poly_kernel(x1, x2)\n",
    "        elif self.kernel == \"rbf\":\n",
    "            return self.rbf_kernel(x1, x2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
    "    \n",
    "    # Compute the full kernel matrix between two datasets\n",
    "    def compute_kernel_matrix(self, X1, X2):\n",
    "        if self.kernel == \"linear\":\n",
    "            return X1 @ X2.T\n",
    "        elif self.kernel == \"poly\":\n",
    "            return (1 + X1 @ X2.T)**self.degree\n",
    "        elif self.kernel == \"rbf\":\n",
    "            dists = np.sum(X1**2, axis=1)[:, np.newaxis] + \\\n",
    "                    np.sum(X2**2, axis=1)[np.newaxis, :] - \\\n",
    "                    2 * (X1 @ X2.T)\n",
    "            return np.exp(-self.rbf_param * dists)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel: {self.kernel}\")\n",
    "\n",
    "    # Plot training loss over iterations\n",
    "    def plot_loss(self):\n",
    "        list_iter = list(range(self.step_iter, self.step_iter * len(self.losses) + 1, self.step_iter))\n",
    "        metrics.plot_loss(list_iter, self.losses)\n",
    "    \n",
    "    # Plot metric evolution over iterations       \n",
    "    def plot_metrics(self):\n",
    "        list_iter = list(range(self.step_iter, self.step_iter * len(self.metric_values) + 1, self.step_iter))\n",
    "        metrics.plot_metrics(list_iter, self.metric_values, self.metric)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f9fd2",
   "metadata": {},
   "source": [
    "# I) SVM\n",
    "\n",
    "## Mathematical formulation of our Support Vector Machine (SVM)\n",
    "\n",
    "We are solving a binary classification problem: we are given training data \n",
    "$(x_1, y_1), \\dots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\{-1, 1\\}$, \n",
    "and we aim to find a classifier that generalizes well.\n",
    "\n",
    "---\n",
    "\n",
    "###  Objective function: Primal SVM (hinge loss + regularization)\n",
    "\n",
    "We aim to find a vector $ w \\in \\mathbb{R}^d $ that minimizes the following objective:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\ \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\max(0;\\ 1 - y_i (\\langle w, x_i \\rangle +b))\n",
    "$$\n",
    "\n",
    "- $ \\lambda $: regularization parameter balancing margin size and classification error  \n",
    "- $ \\max(0,\\ 1 - y_i \\langle w, x_i \\rangle) $: hinge loss function\n",
    "- This term $b$ allows the separation hyperplane to be shifted so that it does not have to pass through the origin.\n",
    "\n",
    "This is exactly the loss implemented in the `compute_loss` method of the `SVM` class.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Subgradient of the Objective (for One Sample)\n",
    "\n",
    "We use **stochastic gradient descent**: at each iteration, we sample randomly one training example \\( (x_i, y_i) \\), and compute the subgradient of the objective.\n",
    "\n",
    "Let:\n",
    "\n",
    "- $ \\ell_i(w, b) = \\max(0, 1 - y_i(\\langle w, x_i \\rangle + b)) $\n",
    "\n",
    "We consider then two cases:\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 1: The hinge loss is active (margin violation)\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "1 - y_i(\\langle w, x_i \\rangle + b) > 0 \\quad \\Leftrightarrow \\quad y_i(\\langle w, x_i \\rangle + b) < 1\n",
    "$$\n",
    "\n",
    "Then the subgradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = \\lambda w - y_i x_i \\\\\n",
    "\\nabla_b J(w) = -y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2: The hinge loss is inactive\n",
    "\n",
    "If:\n",
    "\n",
    "$$\n",
    "y_i(\\langle w, x_i \\rangle + b) \\geq 1\n",
    "$$\n",
    "\n",
    "Then the subgradient is:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w) = \\lambda w \\\\\n",
    "\\nabla_b J(w) = 0\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "This update rule directly follows from the **subgradient of the hinge loss** and the regularization term:\n",
    "\n",
    "- The multiplicative term $ (1 - \\eta_t \\lambda) $ ensures **weight decay**, enforcing margin maximization  \n",
    "- The second term $ \\eta_t y_t x_t $ corrects the weights if the margin is violated  \n",
    "- The bias $ b $ is only updated when a misclassification (or margin violation) occurs\n",
    "###  Pegasos Algorithm (Primal Estimated sub-Gradient Solver for SVM)\n",
    "\n",
    "Then we have the following procedure:\n",
    "\n",
    "1. Sample a data point randomly \\( (x_t, y_t) \\)\n",
    "2. Update the learning rate:  \n",
    "   $ \\eta_t = \\frac{1}{\\lambda t} $\n",
    "3. update $ w $ using the subgradient of hinge loss:\n",
    "    \n",
    "      If $ y_t (\\langle w_t, x_t \\rangle + b) < 1 $:  \n",
    "       $$\n",
    "       w_{t+1} = (1 - \\eta_t \\lambda) w_t + \\eta_t y_t x_t\n",
    "       $$\n",
    "       $$\n",
    "       b_{t+1} = b_t + \\eta_t y_t\n",
    "       $$\n",
    "       Else:  \n",
    "       $$\n",
    "       w_{t+1} = (1 - \\eta_t \\lambda) w_t\n",
    "       $$\n",
    "       $$\n",
    "       b_{t+1} = b_t\n",
    "       $$\n",
    "\n",
    "This is the core of the linear part of the `fit()` method.\n",
    "\n",
    "---\n",
    "\n",
    "##  2) Pegasos Update with Mini-Batch Sampling (Batch Size = m)\n",
    "\n",
    "Instead of updating the model using a single random example at each iteration, we can generalize Pegasos to use a **mini-batch of \\( m \\) random examples**. This often leads to faster and more stable convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### Primal Objective Function (Soft Margin SVM)\n",
    "\n",
    "We still aim to minimize:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\quad \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\max(0,\\ 1 - y_i(\\langle w, x_i \\rangle + b))\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-Batch Pegasos Algorithm\n",
    "\n",
    "Let $ A_t \\subset \\{1, \\dots, n\\} $ be a random subset (mini-batch) of indices of size \\( m \\), sampled at iteration \\( t \\).\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "H_t = \\{ i \\in A_t \\ | \\ y_i (\\langle w_t, x_i \\rangle + b_t) < 1 \\}\n",
    "$$\n",
    "\n",
    "These are the examples in the batch that **violate the margin constraint**.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Approximation with Batch\n",
    "\n",
    "The subgradient estimate becomes:\n",
    "\n",
    "$$\n",
    "\\nabla_w J(w_t) = \\lambda w_t - \\frac{1}{m} \\sum_{i \\in H_t} y_i x_i \\\\\n",
    "\\nabla_b J(w_t) = -\\frac{1}{m} \\sum_{i \\in H_t} y_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Update Rule\n",
    "\n",
    "\n",
    "- 1 Sample a random subset $ A_t \\subset \\{1, \\dots, n\\}$\n",
    "- 2 Let the learning rate be:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\frac{1}{\\lambda t}\n",
    "$$\n",
    "\n",
    "- 3 Update w :\n",
    "\n",
    "    - If $ H_t \\neq \\emptyset $ (some points violate the margin):\n",
    "\n",
    "    $$\n",
    "    w_{t+1} = (1 - \\eta_t \\lambda) w_t + \\frac{\\eta_t}{m} \\sum_{i \\in H_t} y_i x_i \\\\\n",
    "    b_{t+1} = b_t + \\frac{\\eta_t}{m} \\sum_{i \\in H_t} y_i\n",
    "    $$\n",
    "\n",
    "    - If $ H_t = \\emptyset $ (all points satisfy the margin):\n",
    "\n",
    "    $$\n",
    "    w_{t+1} = (1 - \\eta_t \\lambda) w_t \\\\\n",
    "    b_{t+1} = b_t\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Using a mini-batch:\n",
    "\n",
    "- Reduces variance in gradient estimates  \n",
    "- Allows better use of vectorized operations (more efficient computation)\n",
    "- Improves stability and convergence in practice\n",
    "\n",
    "## Tracking the Loss Function in Kernel Pegasos Algorithm\n",
    "\n",
    "To monitor the training progress of the Pegasos algorithm, I use the following loss function :\n",
    "\n",
    "$$\n",
    "\\min_{w,b} \\quad \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\max \\left(0, 1 - y_i (w^\\top x_i + b) \\right)\n",
    "$$\n",
    "\n",
    "This consists of two parts:\n",
    "\n",
    "1. **Hinge loss term**:\n",
    "   $$\n",
    "   \\frac{1}{n} \\sum_{i=1}^n \\max \\left(0, 1 - y_i (w^\\top x_i + b) \\right)\n",
    "   $$\n",
    "   This term penalizes misclassified points or points within the margin by the hinge function. It measures how well the classifier separates the data with a margin.\n",
    "\n",
    "2. **Regularization term**:\n",
    "   $$\n",
    "   \\frac{\\lambda}{2} \\|w\\|^2 = \\frac{\\lambda}{2} w^\\top w\n",
    "   $$\n",
    "   This term controls the complexity of the model by penalizing large weights, helping to prevent overfitting.\n",
    "\n",
    "Therefore, every 20 training iteration, I compute:\n",
    "\n",
    "1. The prediction $ g(x_i) $ for each training example $ x_i $ via the kernel expansion.\n",
    "2. The average hinge loss over all examples.\n",
    "3. The RKHS norm regularization weighted by $\\lambda/2 $.\n",
    "\n",
    "This full loss function allows precise tracking of the objective value decrease and assessing convergence while balancing classification performance and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ba3881",
   "metadata": {},
   "source": [
    "# 3) Kernel Pegasos Algorithm – Mini-Batch Version\n",
    "\n",
    "We extend the Pegasos algorithm to use **mini-batches of size ** $ m $  to update the model in a **Reproducing Kernel Hilbert Space (RKHS)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective Function\n",
    "\n",
    "We aim to minimize the following regularized hinge loss:\n",
    "\n",
    "$$\n",
    "\\min_{f \\in \\mathcal{H}_K} \\quad \\frac{\\lambda}{2} \\| f \\|_{\\mathcal{H}_K}^2 + \\frac{1}{n} \\sum_{i=1}^n \\max\\left(0, 1 - y_i w(x_i)\\right)\n",
    "$$\n",
    "\n",
    "Where in RKHS:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{j=1}^{t-1} \\alpha_j y_j K(x_j, x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel Pegasos Algorithm (Mini-Batch Size \\( m \\))\n",
    "\n",
    "Let:\n",
    "- $ \\lambda > 0$: regularization parameter  \n",
    "- $ \\eta_t = \\frac{1}{\\lambda t} $: learning rate  \n",
    "- $ K(\\cdot, \\cdot) $: kernel function  \n",
    "- $ \\mathcal{S} = \\{(x_i, y_i)\\}_{i=1}^n $: training set\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-Batch Pegasos Algorithm\n",
    "\n",
    "Let $ A_t \\subset \\{1, \\dots, n\\} $ be a random subset (mini-batch) of indices of size \\( m \\), sampled at iteration \\( t \\).\n",
    "\n",
    "Let:\n",
    "\n",
    "$$\n",
    "H_t = \\{ i \\in A_t \\ | \\ y_i (\\langle w_t, x_i \\rangle + b_t) < 1 \\}\n",
    "$$\n",
    "\n",
    "These are the examples in the batch that **violate the margin constraint**.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Approximation with Batch\n",
    "\n",
    "The subgradient estimate becomes:\n",
    "\n",
    "$$\n",
    "\\nabla_w F(g_t) = \\lambda g_t -  \\frac{\\eta_t}{m} \\sum_{i \\in H_t} y_i K(x_i,) \\cdot \\\\\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Steps\n",
    "\n",
    "For $ t = 1, 2, \\ldots$:\n",
    "\n",
    "1. Sample a random subset $ A_t \\subset \\{1, \\ldots, n\\} $ of size $ m $\n",
    "\n",
    "2. Let the learning rate be:\n",
    "\n",
    "$$\n",
    "\\eta_t = \\frac{1}{\\lambda t}\n",
    "$$\n",
    "\n",
    "3. If $ H_t \\neq \\emptyset $, update:\n",
    "\n",
    "$$\n",
    "g_{t+1} = \\left(1 - \\eta_t \\lambda \\right) g_t + \\frac{\\eta_t}{m} \\sum_{i \\in H_t} y_i K(x_i, \\cdot)\n",
    "$$\n",
    "\n",
    "Equivalently, in practice we:\n",
    "- Add the $ x_i $ and $ y_i $ of violating points to the support vectors\n",
    "- Append the corresponding coefficient $ \\alpha_i = \\frac{\\eta_t}{m} $\n",
    "\n",
    "4. If $ H_t = \\emptyset $, perform only the shrinkage:\n",
    "\n",
    "$$\n",
    "g_{t+1} = \\left(1 - \\eta_t \\lambda \\right) g_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##  Early Stopping Criterion\n",
    "\n",
    "We may stop if the update is small:\n",
    "\n",
    "$$\n",
    "\\| \\alpha^{(t)} - \\alpha^{(t-1)} \\| < \\varepsilon\n",
    "$$\n",
    "\n",
    "Where $ \\varepsilon $ is a small threshold.\n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Rule\n",
    "\n",
    "To classify a new input $ x $:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{sign} \\left( \\sum_{j} \\alpha_j y_j K(x_j, x) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Mini-batching improves **stability** and **convergence** over purely stochastic updates.\n",
    "- The number of support vectors can grow, but only when margin violations occur.\n",
    "\n",
    "We will use three kernels:\n",
    "\n",
    "- Linear: $ K(x, x') = x^\\top x' $\n",
    "- RBF (Gaussian):  \n",
    "  $ K(x, x') = \\exp\\left( -\\frac{\\|x - x'\\|^2}{2\\sigma^2} \\right) $\n",
    "- Polynomial:  \n",
    "  $ K(x, x') = (1 + x^\\top x')^d $\n",
    "  \n",
    "## Tracking the Loss Function in Kernel Pegasos Algorithm\n",
    "\n",
    "To monitor the training progress of the Kernel Pegasos algorithm, I use the following complete loss function, which corresponds to the classical SVM objective in a Reproducing Kernel Hilbert Space (RKHS):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(g) = \\frac{1}{n} \\sum_{i=1}^n \\max\\big(0,\\, 1 - y_i g(x_i)\\big) + \\frac{\\lambda}{2} \\|g\\|_{\\mathcal{H}_K}^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ g(x) = \\sum_{j=1}^n \\alpha_j y_j K(x_j, x) $ is the decision function in the RKHS, represented as a weighted sum of kernels between support vectors \\( x_j \\) and input \\( x \\).\n",
    "- The term $\\max(0, 1 - y_i g(x_i))$ is the **hinge loss**, penalizing misclassifications and margin violations.\n",
    "- The regularization term $\\frac{\\lambda}{2} \\|g\\|_{\\mathcal{H}_K}^2$ controls model complexity and is computed as:\n",
    "\n",
    "$$\n",
    "\\|g\\|_{\\mathcal{H}_K}^2 = \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$$\n",
    "\n",
    "This can be efficiently computed using the precomputed Gram matrix \\( K \\) of kernel evaluations between training points.\n",
    "\n",
    "Therefore, every 20 training iteration, I compute:\n",
    "\n",
    "1. The prediction $ g(x_i) $ for each training example $ x_i $ via the kernel expansion.\n",
    "2. The average hinge loss over all examples.\n",
    "3. The RKHS norm regularization weighted by $\\lambda/2 $.\n",
    "\n",
    "This full loss function allows precise tracking of the objective value decrease and assessing convergence while balancing classification performance and model complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a217c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(Algo):\n",
    "\n",
    "    def __init__(self, lambda_=0.01, max_iter=1000, size_subset = 1, tol=1e-2, kernel=None, rbf_param=2.0, degree=2, track = False, step_iter=10, metric = metrics.accuracy):\n",
    "        super().__init__(lambda_, max_iter, size_subset, tol, kernel, rbf_param, degree, track, step_iter, metric)\n",
    "           \n",
    "    def compute_loss(self, X, y):\n",
    "        # loss Pegasos only for no kernel version\n",
    "        hinge_losses = np.maximum(0, 1 - y * (X @ self.w + self.b))\n",
    "        hinge = np.mean(hinge_losses)\n",
    "        reg = 0.5 * self.lambda_ * np.dot(self.w, self.w)\n",
    "        return hinge + reg\n",
    "    \n",
    "    def compute_kernel_loss(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        f = (self.alpha * y) @ self.gram_matrix\n",
    "        hinge_losses = np.maximum(0, 1 - y * f)\n",
    "        hinge = np.mean(hinge_losses)\n",
    "        reg = 0.5 * self.lambda_ * (self.alpha * y) @ self.gram_matrix @ (self.alpha * y)\n",
    "        return hinge + reg\n",
    "    \n",
    "    def stochastic_gradient_descent(self):\n",
    "        n_samples, n_features = self.X_train.shape\n",
    "\n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            w_old = self.w.copy()\n",
    "\n",
    "            eta = 1 / (self.lambda_ * t)\n",
    "\n",
    "            # Random mini-batch of size self.size_subset\n",
    "            indices = np.random.choice(n_samples, self.size_subset, replace=False)\n",
    "            X_batch = self.X_train[indices]\n",
    "            y_batch = self.y_train[indices]\n",
    "\n",
    "            # Compute set of violators H_t\n",
    "            margins = y_batch * (X_batch @ self.w + self.b)\n",
    "            H_t_mask = margins < 1\n",
    "            H_t_X = X_batch[H_t_mask]\n",
    "            H_t_y = y_batch[H_t_mask]\n",
    "\n",
    "            if len(H_t_y) > 0:\n",
    "                self.w = (1 - eta * self.lambda_) * self.w + (eta / self.size_subset) * np.sum(H_t_y[:, None] * H_t_X, axis=0)\n",
    "                self.b = self.b + (eta / self.size_subset) * np.sum(H_t_y)\n",
    "            else:\n",
    "                self.w = (1 - eta * self.lambda_) * self.w\n",
    "                # b remains unchanged\n",
    "\n",
    "            # Tracking loss and metric\n",
    "            if self.track and t % self.step_iter == 0:\n",
    "                loss = self.compute_loss(self.X_train, self.y_train)\n",
    "                self.losses.append(loss)\n",
    "\n",
    "                if self.metric:\n",
    "                    predictions = np.sign(self.X_train @ self.w + self.b)\n",
    "                    score = self.metric(self.y_train, predictions)\n",
    "                    self.metric_values.append(score)\n",
    "\n",
    "            delta_w = np.linalg.norm(self.w - w_old)\n",
    "            norm_w = np.linalg.norm(self.w)\n",
    "            epsilon = 1e-8\n",
    "            if t>10 and norm_w>0 and (delta_w / (norm_w + epsilon ) )< self.tol:\n",
    "                #print(f\"Early stopping \\n\")\n",
    "                break\n",
    "        #print(f\"stop at iteration {t}\")\n",
    "    \n",
    "    def stochastic_gradient_descent_kernel(self):\n",
    "        n_samples = self.X_train.shape[0]\n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            alpha_old = self.alpha.copy()\n",
    "            eta = 1 / (self.lambda_ * t)\n",
    "\n",
    "            # Select random mini-batch\n",
    "            A_t = np.random.choice(n_samples, size=self.size_subset, replace=False)\n",
    "\n",
    "            # Compute prediction for each point in A_t\n",
    "            K_batch = self.gram_matrix[:, A_t]  # Shape (n_samples, batch_size)\n",
    "            f_batch = ((self.alpha * self.y_train) @ K_batch).ravel()  # Shape (batch_size,)\n",
    "            y_batch = self.y_train[A_t].ravel()\n",
    "\n",
    "            # Identify violating examples\n",
    "            H_t = A_t[y_batch * f_batch < 1]\n",
    "\n",
    "            # 1. Apply shrinkage to all alpha values\n",
    "            self.alpha *= (1 - eta * self.lambda_)\n",
    "\n",
    "            # 2. Add contribution of violating examples\n",
    "            self.alpha[H_t] += eta / self.size_subset\n",
    "                \n",
    "            # Tracking\n",
    "            if self.track and t % self.step_iter == 0:\n",
    "                loss = self.compute_kernel_loss(self.X_train, self.y_train)\n",
    "                self.losses.append(loss)\n",
    "                if self.metric is not None:\n",
    "                    y_pred = self.predict(self.X_train)\n",
    "                    score = self.metric(self.y_train, y_pred)\n",
    "                    self.metric_values.append(score)\n",
    "            \n",
    "            # Convergence\n",
    "            if t%20==0:\n",
    "                delta_alpha = np.linalg.norm(self.alpha - alpha_old)\n",
    "                norm_alpha = np.linalg.norm(self.alpha)\n",
    "                epsilon = 1e-8\n",
    "                if norm_alpha>0 and (delta_alpha / (norm_alpha + epsilon)) < self.tol:\n",
    "                    #print(f\"Early stopping \\n\")\n",
    "                    break\n",
    "\n",
    "        #print(f\"stop at iteration {t}\")\n",
    "       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        y = y.astype(float).reshape(-1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Ensure labels are in {-1, 1}\n",
    "        unique_labels = np.unique(y)\n",
    "        if set(unique_labels.flatten()) == {0, 1}:\n",
    "            y = 2 * y - 1  # transform to {-1, 1}\n",
    "        \n",
    "        self.y_train = y.ravel()\n",
    "\n",
    "        if self.kernel is None:\n",
    "            # LINEAR PEGASOS WITH MINI-BATCH\n",
    "            self.w = np.zeros(n_features)\n",
    "            self.b = 0.0\n",
    "            self.stochastic_gradient_descent()\n",
    "           \n",
    "        else:\n",
    "            self.alpha = np.zeros(n_samples)  # Alpha vector\n",
    "            # Precompute kernel matrix\n",
    "            self.gram_matrix = self.compute_kernel_matrix(X, X)\n",
    "            self.stochastic_gradient_descent_kernel()\n",
    "\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        if self.kernel is None:\n",
    "            preds = np.sign(X_test @ self.w + self.b)\n",
    "        else:\n",
    "            support_idx = np.where(self.alpha != 0)[0]\n",
    "            alpha_sv = self.alpha[support_idx]\n",
    "            y_sv = self.y_train[support_idx]\n",
    "            X_sv = self.X_train[support_idx]\n",
    "            K_test = np.array([[self.compute_kernel(x_sv, x_test)\n",
    "                                for x_sv in X_sv] for x_test in X_test])  # shape (n_test, n_sv)\n",
    "\n",
    "            f = K_test @ (alpha_sv * y_sv)  # shape (n_test,)\n",
    "            preds = np.sign(f)\n",
    "        return np.where(preds == 0, 1, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d723fb",
   "metadata": {},
   "source": [
    "# II) Logistic Regression\n",
    "\n",
    "## Mathematical formulation of our Logistic Regression\n",
    "\n",
    "We are solving a binary classification problem: we are given training data \n",
    "$(x_1, y_1), \\dots, (x_n, y_n) \\in \\mathbb{R}^d \\times \\{0, 1\\}$. To align with margin-based methods like SVM, we typically convert the labels to:\n",
    "\n",
    "$y_i \\in \\{−1,+1\\}$.\n",
    "and we aim to find a classifier that generalizes well.\n",
    "\n",
    "---\n",
    "\n",
    "###  1) Objective function: Primal SVM (logistic loss + regularization)\n",
    "\n",
    "We aim to find a vector $ w \\in \\mathbb{R}^d $ that minimizes the following objective:\n",
    "\n",
    "$$\n",
    "\\min_{w} \\ \\frac{\\lambda}{2} \\|w\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\log( 1 + \\exp (- y_i (\\langle w, x_i \\rangle +b)))\n",
    "$$\n",
    "\n",
    "- $ \\lambda $: regularization parameter balancing margin size and classification error  \n",
    "- $ \\log( 1 + \\exp (- y_i (\\langle w, x_i \\rangle +b))) $: logistic loss function\n",
    "- This term $b$ allows the separation hyperplane to be shifted so that it does not have to pass through the origin.\n",
    "\n",
    "This is exactly the loss implemented in the `compute_loss` method of the `LogisticRegression` class.\n",
    "\n",
    "---\n",
    "\n",
    "###  Subgradient of the Objective (for One Sample)\n",
    "\n",
    "Gradient of the Loss (Per Sample)\n",
    "\n",
    "For a single sample $(x_i, y_i)$, define:\n",
    "$$\n",
    "z_i=y_i(w^⊤x_i+b) \n",
    "$$\n",
    "\n",
    "\n",
    "Then, the gradient of the log loss with respect to $w$ and $b$ is:\n",
    "\n",
    "We defined $z_i = y_i(w^t x_i + b)$ and then :\n",
    "\n",
    "$∇_wJ(w)=λw−y_ix_i⋅σ(−z_i)$\n",
    "\n",
    "$∇_bJ(w)=−y_i⋅σ(−z_i)$\n",
    "\n",
    "Where $\\sigma(z)$ is the sigmoid function:\n",
    "    $σ(z)=  \\frac{1}{1+exp(−z)}$\n",
    "    \n",
    "### Stet for Pegasos Update (One Sample)\n",
    "\n",
    "Let $\\eta_t = \\frac{1}{\\lambda t}$ be the learning rate at iteration $t$.\n",
    "\n",
    "At each iteration:\n",
    "\n",
    "1. Sample a data point $(x_t, y_t)$\n",
    "\n",
    "2. Compute $z_t = y_t(w^\\top x_t + b)$\n",
    "\n",
    "3. Compute gradient:\n",
    "$$\n",
    "w_{t+1} = (1 - \\eta_t \\lambda) w_t + \\eta_t y_t x_t \\cdot \\sigma(-z_t) \\\\\n",
    "b_{t+1} = b_t + \\eta_t y_t \\cdot \\sigma(-z_t)\n",
    "$$\n",
    "\n",
    "### Step for Pegasos with Mini-Batch (Batch Size m)\n",
    "\n",
    "Let $A_t \\subset {1, \\dots, n}$ be a batch of $m$ indices, randomly selected.\n",
    "For each $i \\in A_t$, compute $z_i = y_i(w^\\top x_i + b)$\n",
    "Estimate the gradient:\n",
    "$$\n",
    "∇_wJ(w)=λw− \\frac{1}{m} \\sum_{i=1}^m y_i x_i\\sigma(-z_i) \\\\\n",
    "∇_bJ(w)=\\frac{1}{m} \\sum_{i \\in A_t} y_i\\sigma(-z_i)\n",
    "$$\n",
    "\n",
    "    Update rule:\n",
    "$$\n",
    "w_{t+1}=(1−\\frac{t}{\\lambda})w_t+\\frac{η_t}{m} \\sum_{i \\in A_t} y_ix_i\\sigma(−z_i) \\\\\n",
    "b_{t+1}=b_t+\\frac{η_t}{m} \\sum{i \\in A_t} yi\\sigma(−z_i)\n",
    "$$\n",
    "\n",
    "###  Summary\n",
    "\n",
    "- The logistic loss gives probabilistic outputs and is smoother than hinge loss.\n",
    "\n",
    "- Pegasos updates can be adapted to logistic regression by replacing hinge with log loss gradients.\n",
    "\n",
    "- Mini-batches stabilize learning and leverage matrix operations efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56812a8",
   "metadata": {},
   "source": [
    "## 2) Kernel Logistic Regression — Primal Formulation\n",
    "\n",
    "We define the classifier function as a combination of kernels centered at training points:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^n \\alpha_i k(x_i, x)\n",
    "$$\n",
    "\n",
    "Let $f \\in \\mathbb{R}^n$ be the vector of predictions on the training set:\n",
    "$$\n",
    "f = K \\alpha\n",
    "$$\n",
    "where $K \\in \\mathbb{R}^{n \\times n}$ is the Gram matrix with $K_{ij} = k(x_i, x_j)$.\n",
    "\n",
    "We can express the regularized logistic loss directly as a function of $\\alpha$:\n",
    "\n",
    "---\n",
    "\n",
    "## Justification for Using the Regularization Term $\\|\\alpha\\|^2$ Instead of $\\alpha^\\top K \\alpha$\n",
    "\n",
    "In kernelized logistic regression, the regularized objective is often expressed as:\n",
    "\n",
    "$$\n",
    "J(\\alpha) = \\frac{\\lambda}{2} \\alpha^\\top K \\alpha + \\frac{1}{n} \\sum_{i=1}^n \\log\\left(1 + \\exp(-y_i f_i)\\right)\n",
    "$$\n",
    "\n",
    "where $K$ is the kernel (Gram) matrix and $f = K \\alpha$ represents the predictions on the training set.\n",
    "\n",
    "However, it is common in practice to simplify the regularization term to:\n",
    "\n",
    "$$\n",
    "\\frac{\\lambda}{2} \\|\\alpha\\|^2 = \\frac{\\lambda}{2} \\alpha^\\top \\alpha.\n",
    "$$\n",
    "\n",
    "This simplification is justified for several reasons:\n",
    "\n",
    "1. **Computational Efficiency:**  \n",
    "   Computing and manipulating $\\alpha^\\top K \\alpha$ involves the kernel matrix $K$, which is $n \\times n$ and can be large. Using $|\\alpha\\|^2$ reduces computational complexity and memory usage.\n",
    "\n",
    "2. **Bounding the RKHS Norm:**  \n",
    "   Since $K$ is positive semi-definite (PSD), it has a largest eigenvalue $\\lambda_{\\max}(K)$. We have the inequality:\n",
    "\n",
    "$$\n",
    "\\alpha^\\top K \\alpha \\leq \\lambda_{\\max}(K) \\|\\alpha\\|^2\n",
    "$$\n",
    "\n",
    "   meaning that controlling $\\|\\alpha\\|^2$ indirectly controls the RKHS norm $\\alpha^\\top K \\alpha$ up to a constant.\n",
    "\n",
    "3. **Practical Regularization:**  \n",
    "   Regularizing $\\|\\alpha\\|^2$ corresponds to a ridge penalty on the coefficients $\\alpha$, which effectively prevents overfitting by keeping$\\alpha$ small. The hyperparameter $\\lambda$ tunes the regularization strength.\n",
    "\n",
    "4. **Numerical Stability:**  \n",
    "   The matrix $K$ may be ill-conditioned, causing $\\alpha^\\top K \\alpha$ to amplify numerical instabilities. The simpler $\\|\\alpha\\|^2$ regularization is more stable and robust in optimization.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary, using $\\frac{\\lambda}{2} \\|\\alpha\\|^2$ as the regularization term is a practical choice that balances computational efficiency, effective model complexity control, and numerical stability in kernelized logistic regression.**\n",
    "\n",
    "\n",
    "### Primal Objective in Terms of $f$\n",
    "$$\n",
    "J(\\alpha) =  \\frac{\\lambda}{2} \\|\\alpha\\|^2 + \\frac{1}{n} \\sum_{i=1}^n \\log\\big(1 + \\exp(-y_i f_i)\\big)\n",
    "$$\n",
    "\n",
    "- The first term is the $\\ell_2$ regularization\n",
    "- The second term is the empirical logistic loss.\n",
    "- We assume here that $K$ is invertible (or pseudo-inverse is used).\n",
    "where:\n",
    "- $K$ is the kernel matrix: $K_{ij} = K(x_i, x_j)$,\n",
    "- $f_i = \\sum_{j=1}^n \\alpha_j K(x_j, x_i)$.\n",
    "\n",
    "Note: in practice, we work with $\\alpha$ and compute $f = K \\alpha$, so the gradient descent is performed on $\\alpha$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Gradient of the Objective\n",
    "\n",
    "Let’s define the vector:\n",
    "$$\n",
    "s_i = \\sigma(-y_i f_i) = \\frac{1}{1 + \\exp(y_i f_i)}\n",
    "$$\n",
    "\n",
    "Then the gradient of $J(\\alpha)$ is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha J(\\alpha) = \\lambda \\alpha - \\frac{1}{n} \\sum_{i=1}^n y_i s_i K_{:,i}\n",
    "$$\n",
    "\n",
    "Or, in vectorized form:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha J(\\alpha) = \\lambda \\alpha - \\frac{1}{n} K \\left( y \\odot \\sigma(-y \\odot (K \\alpha)) \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\odot$ is the element-wise product,\n",
    "- $y \\in \\mathbb{R}^n$ is the vector of labels $\\in \\{-1, 1\\}$,\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ sigmoid function is applied elementwise.\n",
    "\n",
    "---\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "\n",
    "Let $B_t \\subset \\{1, \\dots, n\\}$ be a mini-batch of size $m$, sampled at iteration $t$.\n",
    "\n",
    "The learning rate is $\\eta_t$.\n",
    "\n",
    "Steps:\n",
    "1. Compute $f_i = (K \\alpha)_i$ for $i \\in B_t$\n",
    "2. Compute $s_i = \\frac{1}{1 + \\exp(y_i f_i)}$\n",
    "3. Gradient estimate:\n",
    "\n",
    "$$\n",
    "\\nabla_\\alpha^{(B_t)} = \\lambda \\alpha - \\frac{1}{m} \\sum_{i \\in B_t} y_i s_i K_{:,i}\n",
    "$$\n",
    "\n",
    "4. Update rule:\n",
    "\n",
    "$$\n",
    "\\alpha \\leftarrow \\alpha - \\eta_t \\nabla_\\alpha^{(B_t)}\n",
    "$$\n",
    "\n",
    "\n",
    "###  Prediction Rule\n",
    "\n",
    "Once trained, the model predicts a label for a new point $x$ via:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{j=1}^n \\alpha_j K(x_j, x) + b\n",
    "$$\n",
    "\n",
    "Then applied the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(f(x)) = \\frac{1}{1 + e^{-f(x)}} \\in (0, 1)\n",
    "$$\n",
    "\n",
    "and then threshold at 0.5:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases}\n",
    "+1 & \\text{if } \\sigma(f(x)) \\geq 0.5 \\\\\n",
    "-1 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407ac604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(Algo):\n",
    "    \n",
    "    def __init__(self, lambda_=0.01, max_iter=1000, size_subset = 1, tol=1e-2, kernel=None, rbf_param=2.0, degree=2, track = False, step_iter=10, metric = metrics.accuracy):\n",
    "        super().__init__(lambda_, max_iter, size_subset, tol, kernel, rbf_param, degree, track, step_iter, metric)\n",
    "\n",
    "    # Sigmoid activation function\n",
    "    def sigmoid(self,X):   \n",
    "        import numpy as np  # just in case\n",
    "        assert callable(np.exp), \"np.exp has been overwritten!\"\n",
    "        X = np.array(X)\n",
    "        X = np.clip(X, -100, 100)\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    \n",
    "    # Numerically stable version of log(1 + exp(x))\n",
    "    def log1pexp(self, x):\n",
    "        x = np.clip(x, -100, 100) \n",
    "        return np.where(x > 0, x + np.log1p(np.exp(-x)), np.log1p(np.exp(x)))\n",
    "    \n",
    "    # Compute regularized logistic loss (linear version)\n",
    "    def compute_loss(self, X_b):\n",
    "        \"\"\"\n",
    "        Compute regularized logistic loss for labels in {-1, 1}.\n",
    "        X_b : numpy array, shape (n_samples, n_features + 1), input data with bias term\n",
    "        returns: scalar, regularized logistic loss\n",
    "        \"\"\"\n",
    "        n_samples = self.X_train.shape[0]\n",
    "        logits = X_b @ self.w\n",
    "        yz = self.y_train * logits\n",
    "        loss_vec = np.log(1 + np.exp(-yz))\n",
    "        loss_logistic = np.mean(loss_vec)\n",
    "\n",
    "        reg_term = (self.lambda_ / 2) * np.sum(self.w[1:] ** 2)\n",
    "        return loss_logistic + reg_term\n",
    "        \n",
    "    # Compute regularized logistic loss in kernel space    \n",
    "    def compute_loss_kernel(self):\n",
    "        \"\"\"\n",
    "        Compute regularized logistic loss in kernel space for labels in {-1, 1}.\n",
    "        \"\"\"\n",
    "        n_samples = self.y_train.shape[0]\n",
    "        K = self.gram_matrix.copy()\n",
    "        \n",
    "        # Compute raw scores f(x_i) = K @ alpha\n",
    "        f = K @ self.alpha\n",
    "        yz = self.y_train * f\n",
    "        loss_vec = self.log1pexp(-yz)\n",
    "        loss_logistic = np.mean(loss_vec)\n",
    "\n",
    "        # norm regularization\n",
    "        regularization = (self.lambda_ / 2) * np.linalg.norm(self.alpha)**2\n",
    "        \n",
    "        return loss_logistic + regularization\n",
    "\n",
    "    # SGD for linear logistic regressio\n",
    "    def stocashtic_gradient_descent(self,X_b):\n",
    "        \"\"\"\n",
    "        stocashtic_gradient_descent.\n",
    "\n",
    "        X_b : numpy array, shape (n_samples, n_features + 1), input data with bias term\n",
    "        \"\"\"\n",
    "        n_samples = X_b.shape[0]\n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            w_old = self.w.copy()\n",
    "            eta = 1 / (self.lambda_ * t)\n",
    "            \n",
    "            # Mini-batch\n",
    "            indices = np.random.choice(n_samples, self.size_subset, replace=False)\n",
    "            X_batch = X_b[indices]\n",
    "            y_batch = self.y_train[indices]\n",
    "            \n",
    "            # Predictions\n",
    "            z = X_batch @ self.w\n",
    "            yz = y_batch * z\n",
    "            sigmoid_term = self.sigmoid(-yz)  # shape (batch_size, 1)\n",
    "            \n",
    "            # Gradient of logistic loss\n",
    "            grad = -(X_batch.T @ (y_batch * sigmoid_term)) / self.size_subset\n",
    "            \n",
    "            # Regularisation term (excluding bias b)\n",
    "            reg = self.lambda_ * self.w\n",
    "            reg[0] = 0\n",
    "\n",
    "            self.w -= eta * (grad + reg)\n",
    "            \n",
    "            # Tracking loss and metrics\n",
    "            if self.track and t % self.step_iter == 0:\n",
    "                loss = self.compute_loss(X_b)\n",
    "                self.losses.append(loss)\n",
    "                if self.metric is not None:\n",
    "                    preds = self.predict(self.X_train)\n",
    "                    score = self.metric(self.y_train, preds)\n",
    "                    self.metric_values.append(score)\n",
    "            \n",
    "            delta_w = np.linalg.norm(self.w - w_old)\n",
    "            norm_w = np.linalg.norm(self.w)\n",
    "            epsilon = 1e-8\n",
    "            if t>10 and norm_w>0 and delta_w / (norm_w + epsilon ) < self.tol:\n",
    "                #print(f\"Early stopping \\n\")\n",
    "                break\n",
    "        #print(f\"stop at iteration {t}\")\n",
    "\n",
    "    # SGD for kernel logistic regression    \n",
    "    def stochastic_gradient_descent_kernel(self):\n",
    "        n_samples = self.X_train.shape[0]\n",
    "        for t in range(1, self.max_iter + 1):\n",
    "            alpha_old = self.alpha.copy()\n",
    "            eta = 1 / (self.lambda_ * t)\n",
    "\n",
    "            indices = np.random.choice(n_samples, self.size_subset, replace=False)\n",
    "            K_batch = self.gram_matrix[indices, :]  # (batch_size, n_samples)\n",
    "            y_batch = self.y_train[indices].reshape(-1, 1)  # (batch_size, 1)\n",
    "\n",
    "            f = K_batch @ self.alpha  # (batch_size, 1)\n",
    "            yz = y_batch * f  # (batch_size, 1)\n",
    "            sigmoid_term = self.sigmoid(-yz)  # (batch_size, 1)\n",
    "\n",
    "            # gradient w.r.t. α: (n_samples, 1)\n",
    "            grad = -(K_batch.T @ (y_batch * sigmoid_term)) / self.size_subset\n",
    "\n",
    "            reg = self.lambda_ * (self.alpha)\n",
    "            self.alpha -= eta * (grad + reg)\n",
    "            if np.any(np.isnan(self.alpha)) or np.any(np.isinf(self.alpha)):\n",
    "                print(\"Numerical instability detected at iteration\", t)\n",
    "                break\n",
    "\n",
    "            # tracking\n",
    "            if self.track and t % self.step_iter == 0:\n",
    "                loss = self.compute_loss_kernel()\n",
    "                self.losses.append(loss)\n",
    "                if self.metric is not None:\n",
    "                    preds = self.predict(self.X_train)\n",
    "                    score = self.metric(self.y_train, preds)\n",
    "                    self.metric_values.append(score)\n",
    "\n",
    "            # convergence      \n",
    "            if t%20:\n",
    "                delta_alpha = np.linalg.norm(self.alpha - alpha_old)\n",
    "                norm_alpha = np.linalg.norm(self.alpha)\n",
    "                epsilon = 1e-8\n",
    "                if norm_alpha>0 and (delta_alpha / (norm_alpha + epsilon)) < self.tol:\n",
    "                    #print(\"Early stopping\\n\")\n",
    "                    break\n",
    "        #print(f\"stop at iteration {t}\")\n",
    "    \n",
    "    # Fit model to data\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit function.\n",
    "\n",
    "        X : numpy array, shape (n_samples, n_features ), input data without bias term b\n",
    "        y   : numpy array, shape (n_samples, 1), target labels (0 or 1)\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        y = y.astype(float).reshape(-1, 1)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Ensure labels are in {-1, 1}\n",
    "        unique_labels = np.unique(y)\n",
    "        if set(unique_labels.flatten()) == {0, 1}:\n",
    "            y = 2 * y - 1  # transform to {-1, 1}\n",
    "        \n",
    "        self.y_train = y\n",
    "        \n",
    "        #linear model\n",
    "        if self.kernel is None:\n",
    "            # Add bias b to X \n",
    "            X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "            #y = \n",
    "\n",
    "            # Initialization of w (with b included)\n",
    "            self.w = np.zeros((n_features + 1, 1))\n",
    "            self.stocashtic_gradient_descent(X_b)\n",
    "        # Kernel logistic regression\n",
    "        else:\n",
    "            # Compute Gram matrix\n",
    "            self.gram_matrix = self.compute_kernel_matrix(X, X)\n",
    "            self.alpha = np.zeros((n_samples, 1))  # coefficients alpha\n",
    "\n",
    "            # launch stochastic gradient on alpha\n",
    "            self.stochastic_gradient_descent_kernel()\n",
    "      \n",
    "    # Predict probabilities for labels 1\n",
    "    def predict_proba(self, X_test):\n",
    "        n_samples = X_test.shape[0]\n",
    "        if self.kernel is None:\n",
    "            X_b_test = np.c_[np.ones((n_samples, 1)), X_test]\n",
    "            logits = X_b_test @ self.w\n",
    "        else:\n",
    "            K_test = self.compute_kernel_matrix(self.X_train, X_test)\n",
    "            logits = K_test.T @ self.alpha\n",
    "\n",
    "        probs = self.sigmoid(logits).ravel()\n",
    "\n",
    "        return probs\n",
    "        \n",
    "    # Predict class labels\n",
    "    def predict(self, X_test):\n",
    "        n_samples = X_test.shape[0]\n",
    "        if self.kernel is None:\n",
    "            X_b_test = np.c_[np.ones((n_samples, 1)), X_test]\n",
    "            logits = X_b_test @ self.w\n",
    "        else:\n",
    "            K_test = self.compute_kernel_matrix(self.X_train, X_test)\n",
    "            logits = K_test.T @ self.alpha\n",
    "\n",
    "        probs = self.sigmoid(logits).ravel()\n",
    "        \n",
    "        return np.where(probs >= 0.5, 1, -1)\n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
